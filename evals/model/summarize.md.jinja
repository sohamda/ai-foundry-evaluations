
# Average scores

> [!NOTE]
> The evaluation results for each test variant are displayed below, averaged over the {{ tests|length }} test cases.

| Evaluator | |
| :-------- | ---:|
{% for evaluator_name in evaluator_names -%}
| {{ evaluator_name }} | {{ average_eval_scores[evaluator_name] }} 
{% endfor %}

# Individual test scores

{% for (query, ground_truth), rows in tests.items() %}

## Test {{ loop.index }}

### Inputs

* **Query:** {{ query }}
* **Ground truth:** {{ ground_truth }}

### Eval Scores

| Evaluator | |
| :-------- | {{ " ---: |" * (rows|length) }}
{% for evaluator_name in evaluator_names -%}
| {{ evaluator_name }} | {% for row in rows -%} {{ row.eval_scores[evaluator_name] }} |{% endfor %}
{% endfor %}

{% if show_raw_output %}

### Raw Outputs

{% for row in rows -%}
<details>

{{ row['inputs']['response'] }}

</details>

{% endfor %}
{% endif %}
{% endfor %}